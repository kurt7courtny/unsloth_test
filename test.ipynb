{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def float32_to_bf16_kernel(\n",
    "    input_ptr, \n",
    "    output_ptr, \n",
    "    n_elements, \n",
    "    BLOCK: tl.constexpr,\n",
    "    BF16ASM: tl.constexpr\n",
    "):\n",
    "    # 计算当前线程块处理的偏移量\n",
    "    pid = tl.program_id(axis=0)\n",
    "    # 创建一个块内的偏移量\n",
    "    block_offset = tl.arange(0, BLOCK)\n",
    "    # 计算全局偏移量\n",
    "    offsets = pid * BLOCK + block_offset\n",
    "    \n",
    "    # 检查是否超出边界\n",
    "    mask = offsets < n_elements\n",
    "    \n",
    "    # 加载float32数据\n",
    "    x = tl.load(input_ptr + offsets, mask=mask)\n",
    "    \n",
    "    # 使用inline assembly将float32转换为bf16\n",
    "    if BF16ASM:\n",
    "        x = tl.inline_asm_elementwise(\n",
    "            asm=\"\"\"\n",
    "            {\n",
    "                .reg .b32 tmp, round_bit, mantissa, sign_exp, rounding_bias;\n",
    "                .reg .b16 result;\n",
    "                .reg .pred p;\n",
    "            \n",
    "                // 获取原始32位浮点值\n",
    "                mov.b32 tmp, $1;\n",
    "                \n",
    "                // 提取符号位和指数位\n",
    "                and.b32 sign_exp, tmp, 0xFF800000;\n",
    "                \n",
    "                // 提取尾数位\n",
    "                and.b32 mantissa, tmp, 0x007FFFFF;\n",
    "                \n",
    "                // 获取要舍入的位 (第16位)\n",
    "                and.b32 round_bit, mantissa, 0x00008000;\n",
    "                \n",
    "                // 检查是否需要进行舍入 (round-to-nearest-even)\n",
    "                // 如果第16位是1，我们需要考虑舍入\n",
    "                setp.eq.u32 p, round_bit, 0x00008000;\n",
    "                // 如果是奇数（第17位是1）或者低位有非零值，则进位\n",
    "                and.b32 rounding_bias, mantissa, 0x0000FFFF;\n",
    "                @p add.u32 rounding_bias, rounding_bias, 0x00008000;\n",
    "                and.b32 rounding_bias, rounding_bias, 0x00010000;\n",
    "                \n",
    "                // 添加舍入偏移量到原始值\n",
    "                add.u32 tmp, tmp, rounding_bias;\n",
    "                \n",
    "                // 提取高16位（符号、指数和高位尾数）\n",
    "                shr.b32 tmp, tmp, 16;\n",
    "                \n",
    "                // 转换为16位并存储结果\n",
    "                cvt.u16.u32 result, tmp;\n",
    "                mov.b16 $0, result;\n",
    "            }\n",
    "            \"\"\",\n",
    "            constraints=(\"=h, r\"),  # h表示16位寄存器，r表示32位寄存器\n",
    "            args=[x],\n",
    "            dtype=(tl.bfloat16),  # 输出类型为bfloat16\n",
    "            is_pure=True,\n",
    "            pack=1,\n",
    "        )\n",
    "    else:\n",
    "        x += 1\n",
    "    # 存储结果\n",
    "    tl.store(output_ptr + offsets, x, mask=mask)\n",
    "\n",
    "def float32_to_bf16(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    将float32 tensor转换为bfloat16 tensor\n",
    "    \n",
    "    Args:\n",
    "        x: 输入的float32 tensor\n",
    "    \n",
    "    Returns:\n",
    "        bfloat16 tensor\n",
    "    \"\"\"\n",
    "    # 确保输入tensor在GPU上\n",
    "    assert x.is_cuda and x.dtype == torch.float32, \"输入必须是CUDA float32 tensor\"\n",
    "    \n",
    "    # 创建输出tensor\n",
    "    y = torch.empty(x.shape, dtype=torch.bfloat16, device=x.device)\n",
    "    \n",
    "    # 获取元素总数\n",
    "    n_elements = x.numel()\n",
    "    \n",
    "    # 定义线程块大小\n",
    "    BLOCK = 1024\n",
    "    \n",
    "    # 计算需要启动的线程块数量\n",
    "    grid = ((n_elements + BLOCK - 1) // BLOCK,)\n",
    "    \n",
    "    # 启动kernel\n",
    "    float32_to_bf16_kernel[grid](\n",
    "        x,\n",
    "        y,\n",
    "        n_elements,\n",
    "        BLOCK,\n",
    "        True\n",
    "    )\n",
    "    \n",
    "    return y\n",
    "\n",
    "def compare_conversion_methods(x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    比较我们的triton实现和PyTorch原生实现的结果\n",
    "    \n",
    "    Args:\n",
    "        x: 输入的float32 tensor\n",
    "    \"\"\"\n",
    "    # 使用我们的triton kernel进行转换\n",
    "    triton_result = float32_to_bf16(x)\n",
    "    \n",
    "    # 使用PyTorch内置方法进行转换\n",
    "    pytorch_result = x.to(torch.bfloat16)\n",
    "    \n",
    "    # 比较Triton和PyTorch结果\n",
    "    torch.testing.assert_close(triton_result, pytorch_result, atol=1e-6, rtol=1e-6)\n",
    "    triton_vs_pytorch = torch.all(triton_result == pytorch_result)\n",
    "    triton_vs_pytorch_max_diff = torch.max(torch.abs(triton_result.float() - pytorch_result.float())).item()\n",
    "    \n",
    "    print(\"转换结果比较:\")\n",
    "    print(f\"Triton实现 vs PyTorch: 是否相同={triton_vs_pytorch}, 最大差异={triton_vs_pytorch_max_diff}\")\n",
    "    \n",
    "    # 打印部分结果进行对比\n",
    "    n_samples = min(10, x.numel())\n",
    "    print(\"\\n示例对比:\")\n",
    "    print(\"索引 | 原始float32 | Triton bf16 | PyTorch bf16\")\n",
    "    print(\"-\" * 60)\n",
    "    for i in range(n_samples):\n",
    "        idx = i\n",
    "        orig = x.flatten()[idx].item()\n",
    "        triton_val = triton_result.flatten()[idx].float().item()\n",
    "        pytorch_val = pytorch_result.flatten()[idx].float().item()\n",
    "        print(f\"{idx:4d} | {orig:12.6f} | {triton_val:12.6f} | {pytorch_val:12.6f}\")\n",
    "\n",
    "def performance_test(x: torch.Tensor, n_iterations=100):\n",
    "    \"\"\"\n",
    "    比较不同实现的性能\n",
    "    \n",
    "    Args:\n",
    "        x: 输入的float32 tensor\n",
    "        n_iterations: 重复次数，用于测量平均性能\n",
    "    \"\"\"\n",
    "    print(\"\\n性能测试:\")\n",
    "    \n",
    "    # 预热GPU\n",
    "    for _ in range(10):\n",
    "        _ = float32_to_bf16(x)\n",
    "        _ = x.to(torch.bfloat16)\n",
    "    \n",
    "    # 同步GPU\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # 测试Triton实现\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    for _ in range(n_iterations):\n",
    "        _ = float32_to_bf16(x)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    triton_time = start.elapsed_time(end) / n_iterations\n",
    "    \n",
    "    # 测试PyTorch内置实现\n",
    "    start.record()\n",
    "    for _ in range(n_iterations):\n",
    "        _ = x.to(torch.bfloat16)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    pytorch_time = start.elapsed_time(end) / n_iterations\n",
    "    \n",
    "    print(f\"Triton实现: {triton_time:.4f} ms\")\n",
    "    print(f\"PyTorch实现: {pytorch_time:.4f} ms\")\n",
    "    \n",
    "    # 计算相对性能\n",
    "    if pytorch_time > 0 and triton_time > 0:\n",
    "        ratio = pytorch_time / triton_time\n",
    "        if ratio > 1:\n",
    "            print(f\"Triton比PyTorch快 {ratio:.2f}x\")\n",
    "        else:\n",
    "            print(f\"PyTorch比Triton快 {1/ratio:.2f}x\")\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    # 检查是否有可用的CUDA设备\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"错误：未检测到CUDA设备，此示例需要GPU才能运行\")\n",
    "        return\n",
    "    \n",
    "    # 设置随机种子以确保可重复性\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # 创建测试数据\n",
    "    # 包含各种类型的值\n",
    "    special_values = torch.tensor([\n",
    "        # 正常值\n",
    "        1.0, -1.0, 3.14159, -2.71828, \n",
    "        # 大值和小值\n",
    "        1e20, -1e20, 1e-20, -1e-20,\n",
    "        # 特殊值\n",
    "        float('inf'), float('-inf'), float('nan'),\n",
    "        # 0及接近0的值\n",
    "        0.0, -0.0, 1e-30, -1e-30\n",
    "    ], dtype=torch.float32, device='cuda')\n",
    "    \n",
    "    # 添加一些随机值\n",
    "    random_values = torch.randn(10000000, device='cuda')\n",
    "    \n",
    "    # 合并特殊值和随机值\n",
    "    x = torch.cat([special_values, random_values])\n",
    "    \n",
    "    print(f\"测试数据大小: {x.shape}\")\n",
    "    \n",
    "    # 比较转换方法\n",
    "    compare_conversion_methods(x)\n",
    "    \n",
    "    # 性能测试\n",
    "    # 创建一个更大的tensor用于性能测试\n",
    "    perf_x = torch.randn(10_000_000, device='cuda')\n",
    "    performance_test(perf_x)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
